\chapter{Linearity}

Given a function $\mathbf{f}(x) \colon x \to y$, and some constant $c$, we can ask if it matters whether we multiply the input $x$ by $c$ and then apply the function to the result, or we apply the function to $x$ and multiply the result by $c$. That is, does this equation hold?

$$\mathbf{f}(cx) = c\mathbf{f}(x)$$

Or to put it another way, if we "scale up" the input, does the output scale up in proportion?

Also for two variables $x$ and $y$:

$$\mathbf{f}(x + y) = \mathbf{f}(x) + \mathbf{f}(y)$$

That is, it may be that it doesn't matter whether we sum the inputs and then apply the function to the sum, or apply the function to each input and then sum the results.

If both these equations hold, we say $\mathbf{f}$ is linear.

Actually if you take the addition rule and set $y = x$:

$$\mathbf{f}(x + x) = \mathbf{f}(x) + \mathbf{f}(x)$$

or:

$$\mathbf{f}(2x) = 2\mathbf{f}(x)$$

Which is surely a huge clue about the scaling rule! Though neither is a complete statement of linearity without the other.

Sometimes these are combined into a single, albeit more confusing, requirement:

$$\mathbf{f}(ax + by) = a\mathbf{f}(x) + b\mathbf{f}(y)$$

We can generalise this concept beyond functions that act on numbers. Think of $\mathbf{f}$ as an operator. The objects it operates on can be of any type for which we can define addition and scaling (multiplication by a constant), as that's all we need to check the linearity requirement.

We can define these capabilities for vectors, matrices and indeed all tensors, so operators acting on all those things can be linear. Now, it's easy to see how this might happen, because all those things can be described by scalar components, which can themselves be added and scaled.

So let's consider something way more abstract. It's also commonplace to define addition for functions (forget about our previous use of $\mathbf{f}$):

$$\mathbf{h} = \mathbf{f} + \mathbf{g}$$

The sum of two functions is another function, one whose value is the sum of the values of the other two functions for the same input:

$$\mathbf{h}(x) = \mathbf{f}(x) + \mathbf{g}(x)$$

And similarly we can scale a function, to make another function:

$$\mathbf{h} = k \mathbf{f}$$

$$\mathbf{h}(x) = k \,\mathbf{f}(x)$$

If we encounter an operator $\hat{O}$ that somehow acts on a function to produce another function, we can ask if $\hat{O}$ is linear. That is:

$$\hat{O}(\mathbf{f} + \mathbf{g}) = \hat{O}(\mathbf{f}) + \hat{O}(\mathbf{g})$$

is true, as is:

$$\hat{O}(k\mathbf{f}) = k\,\hat{O}(\mathbf{f})$$

Note that an operator is not restricted to mappings that perform arithmetic on parameters. An operator may dig into the \textit{definition} of a function and transform it through analysis (in coding terms, an operator can read the source of the input function, not merely call it.)

So an example of an operator would be differentiation. A function such as $\sin$ can be differentiated analytically and the result is $\cos$. If we differentiate $\cos$ we get $-\sin$. It doesn't matter if we:

\begin{itemize}    
    \item add the functions, then differentiate, or
    \item differentiate the functions, then add
\end{itemize}

Either way, we end up with $\cos - \sin$. This is true whatever functions we're adding, because differentiation works on each term individually and then adds the results.

The same goes for scaling, because when you amplify a function, you amplify the slope of the function.

> Of course, by scaling a function we mean multiplying it by a \textit{constant}; if we multiplied $f(x)$ by another function $g(x)$, the gradient curve could end up with a wildly different shape. If we differentiate $f(x)$ and then multiply it by $g(x)$, we've skipped the differentiation of $g$.

So, the "differentiation operator" meets the requirements of linearity, so differentiation is linear (and intuitively as integration is the inverse operation of differentiation, it too must be linear).

Another example is the Fourier transform, $\mathcal{F}$. If you add two waves and take the Fourier transform of the combined wave, you get the same frequency distribution as if you took the Fourier transform of each wave separately and then added the two frequency distributions:

$$\mathcal{F} (\mathbf{g} + \mathbf{h}) = \mathcal{F} (\mathbf{g}) + \mathcal{F} (\mathbf{h})$$

And unsurprisingly, it's the same story with scaling:

$$\mathcal{F} (k \mathbf{g}) = k \, \mathcal{F} (\mathbf{g})$$

(And the same for $\mathcal{F}^{-1}$ as you'd expect.)

This next one is a little looser as an analogy. We can classify all objects in a binary way, dividing them into members and non-members of some set. Suppose we come up with a sense in which we can add two members of the set, or scale them. Is the result always a member of the set also? If so, that's a kind of linearity.

For example, if two functions are solutions to the Schroedinger equation with some potential, they be scaled and added to produce a third solution, so we say the Schroedinger equation is linear.
