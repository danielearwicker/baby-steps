\chapter{Tensors}

\section{Einstein Notation}

Where we sum over terms where the summation variable (in this case $n$) appears twice, once as superscript and once as subscript, which happens a lot, Einstein got tired of writing the summation symbol so many times and so adopted the convention of omitting it:

$$C^i_j = A^i_nB^n_j$$

Note that where we apply this to transformations on a geometric space, the number of dimensions determines the range of every loop variable. So in 3 dimensions, $i$, $j$ and $n$ are all $\in {1, 2, 3}$.

Also bear in mind that the distinction between subscript and superscript indices is going to be important, and in Einstein's summation notation the repeated variable in a term must appear once as a subscript and once as a superscript index, as will become clear later on.

The summation loop variable $n$ (known as a \textit{bound} or \textit{dummy} variable) is clearly different from the other two variables. We are not summing over $i$ or $j$, because we are preserving separate matrix cell values $C^i_j$ in the output, not mixing them together.

Einstein's notation always makes it unambigious what operations we are performing. if there are no repeated indices, there is no summation: we're doing the tensor product.

$$C^i_j = A^iB_{j}$$

This literally just tells us how to compute the element $(i, j)$ of $\boldsymbol{C}$, by multiplying the $i$-th element of $\boldsymbol{A}$ and the $j$-th element of $\boldsymbol{B}$.

If there is at least one repeated element, we're doing contraction, i.e. summation:

$$E^i = C^i_jD^j$$

This is telling us how to compute the $i$-th element of $\boldsymbol{E}$, by summing over $j$ the element $(i, j)$ of $\boldsymbol{C}$ multiplied by the $j$-th element of $\boldsymbol{D}$.

So there's never any ambiguity, but also it's not necessary to say whether you're doing the tensor product or contraction.

\section{Tensor Product}

Suppose rather than a square matrix, we had a cubic latice, and thus needed three loop variables, $i$, $j$, $k$ to address each element. We can picture this as a cube and write its elements as $A_{ijk}$ (we have arbitrarily used subscript indices - that distinction is irrelevant here).

We can make the outer product $\otimes$ (also known as the tensor product) between this cube and a square matrix $B_{pq}$ by pairing up every combination of their elements and multiplying them.

$$C_{ijkpq} = A_{ijk}B_{pq}$$

This means that $C$ is a 5-dimensional object which seems difficult to picture at first, but there is a completely obvious way to think about it: imagine a square grid addressed by $p$, $q$, and in each cell is a small cube of numbers addressed by $i$, $j$, $k$. So the full address of each number requires five numbers, $p$ and $q$ to find a grid cell and then $i$, $k$, $k$ to locate a cell of the cube found within that grid cell. The number in each piece of a cube (and that cube sitting a grid cell) is the product of the corresponding cells in the original $A$ cube and the $B$ grid.

By the way, we avoid calling it 5-dimensional (even though that is an accurate description of the structure) because we already use dimension to refer to the range of each index: $i \in {1, 2, 3}$ would mean three dimensions, even though we have 5 indices like that. So we say the number of indices is the \textit{rank} of the tensor.

Note that in geometrical tensors we are frequently only interested in index variables that are all of the same dimension: squares, cubes, and so on, because they relate to the dimensionality of the geometric space.

\section{Superscript and Subscript Indices}

When considering matrices (which is to say, rank-2 tensors) we think of the superscript index as the row and the subscript as the column. So a column vector's coordinates are superscripted, whereas a row vector's are subscripted. And so a matrix can be thought of as a set of column vectors side by side, or a set of row vectors stacked in layers.

This is not a meaningful rule with higher rank tensors.

In fact the true rule is that a contravariant index is superscript while a covariant index is subscript. And then the convention with matrices is that rows are covariant and columns are contravariant. This matches up with our convention of writing ordinary vectors (contravariant) as columns and covectors (covariant) as rows.

A matrix can be thought of as a stack of row vectors. Each row defines a function for extracting a coordinate from a column vector, relative to a basis vector.

We can quite happily produce a tensor product mess such as:

$$C^{ip}_{jkq} = A^{i}_{jk}B^{p}_{q}$$

In the tensor product, if an index is superscript in a source tensor, it remains a superscript in the output tensor, and the same for subscripts.

The various indices have each been arbitrarily thrown into one of the two available locations. In terms of the arithmetical machinery they all behave identically, but preserving these two types ensures that the resulting tensor will be transformable under a change of basis.

\section{Tensor Contraction}

In the tensor product there are no repeated indices in the terms so Einstein notation tells us there is no summation.

To introduce summation will mean collapsing cells together, reducing the rank of the structure. This allows us to arrive at the dot product in a roundabout, two-stage process.

If we do the tensor product on two rank-1 tensors, $A_i$ and $B^j$, the result $C^j_i$ is a a rank-2 tensor (a matrix).

$$A \otimes B = C^j_i = A_iB^j$$

We've been careful here to follow a convention in assigning superscript and subscript indices so $A_i$ is a row and $i$ denotes the column within that row, and $B^j$ is a column and $j$ denotes the row within that column. We then always place the row on the left and the column on the right.

To obtain the dot product, we throw away everything but the main diagonal of that grid, all the elements where $i=j$, and we sum those elements. The Kroneker delta expresses this:

$$A \cdot B = \sum_i\sum_j{A_iB^j\delta_{ij}}$$

But equally, thanks to Einstein notation, we can think of this as using a single index variable:

$$A \cdot B = A_iB^i$$

Because $i$ appears twice in the term (once each as a subscript and a superscript), this is a summation of the diagonal elements.

The result is a single scalar value, also known as a rank-0 tensor. So the above process (known as \textit{contraction}) has reduced the rank by 2, and this in fact is what always happens regardless of the rank of the tensors involved.

The only rule is that the contraction must involve one subscript and one superscript index. The arithmetic will blindly work if this rule is broken, but the result will not have geometrical meaning.

We can arrive at matrix multiplication in the same way. If we do the tensor product on two rank-2 tensors (matrices), $A^i_j$ and $B^p_q$, the result $C^{ip}_{jq}$ is a rank-4 tensor (a grid of grids?)

$$A \otimes B = C^{ip}_{jq} = A^i_jB^p_q$$

Let's perform contraction C by replacing $p$ with $j$ (this is a valid thing to do because $j$ is a subscript and $p$ is a superscript, so they are the opposite kinds of index; we could have chosen $i$ and $q$ instead).

$$C^i_q = A^i_jB^j_q$$

As $j$ is now a repeated (dummy) index, this is a summation. We're doing something similar to taking the diagonal of a matrix, and we can reorganise our visualisation $C^{ip}_{jq}$ to something much simpler than a hypercube, via the trick of thinking about nested grids.

Picture a square grid addressed by $i$, $q$, and in each of its cells there is another grid of numbers, addressed by $j$, $p$, and those numbers are each the product $A^i_jB^p_q$. So when we say that $p = j$, we're literally taking a diagonal of each of the nested grids, to get a set of numbers $A^i_jB^j_q$ that we sum together, so we end up with a single number in each cell of the outer grid, and it becomes merely a square grid of numbers, $C^i_q$.

Note how once again the raw tensor product saw its rank fall by two, from four to two. Also note that the combination of tensor product between two rank-2 tensors, followed by a contraction, to get a new rank-2 tensor, is arithmetically equivalent to matrix multiplication. We can even (although this is rarely applicable) relax the rule that all the indices must be of the same dimension, and only require the indices that we contract to be the same dimension (as of course they must be, otherwise how would they pair up?)

So we've found a generalisation of the dot product and matrix multiplication that extends to tensors of any rank.

Note that in the definition of matrix multiplication, we combine the tensor product and reduction into a single operation, which saves some effort, but it also disguises something: it appears we never have to choose which indices to eliminate in the contraction. Matrix multiplication has a built-in decision to eliminate two specific indices.

This comes from the fact that the matrix indices are classified as either rows (superscript) or columns (subscript), and one matrix is on the left and the other on the right. So multiplying two square matrices $\boldsymbol{A}$ and $\boldsymbol{B}$, to obtain $\boldsymbol{C}$ we use the same index for the subscript on the left and the superscript on the right:

$$C^i_j = A^i_nB^n_j$$

To spell this out:

$$
\begin{bmatrix}C^1_1 & C^1_2 \\ C^2_1 & C^2_2\end{bmatrix}
= \begin{bmatrix}A^1_nB^n_1 & A^1_nB^n_2 \\ A^2_nB^n_1 & A^2_nB^n_2\end{bmatrix}
= \begin{bmatrix}A^1_1B^1_1 + A^1_2B^2_1 & A^1_1B^1_2 + A^1_2B^2_2 \\ A^2_1B^1_1 + A^2_2B^2_1 & A^2_1B^1_2 + A^2_2B^2_2\end{bmatrix}
$$

If we instead used the same index for the left superscript and the right subscript, we wouldn't get the same answer.

$$A^n_iB^j_n \ne A^i_nB^n_j$$

But if we then reverse the order of the matrices, which does not affect the answer, we restore the "left subscript, right superscript" rule:

$$A^n_iB^j_n = B^j_nA^n_i$$

And this is exactly what we find with matrix multiplication. By interchanging the two input matrices, we effectively change the decision as to which indices to eliminate in the contraction, and that is why matrix multiplication is noncommutative.

\section{Tensors as geometric objects}

The point of tensors is to produce the same value from a computation regardless of the coordinate system chosen. This means the tensor is a geometric object: its description in terms of coordinates is not fundamental. It has magnitude and direction (if rank-2 it has two directions, and so on.)

It also means that for a tensor there is a rule governing how its coordinates must change under a change of basis. The whole point of this rule is to ensure that the change of basis does not affect the result.

If we treat a rank N tensor as a scalar-valued function of N vectors, some changes of basis will make no difference to the result, but others will. Those transformations to the basis under which the scalar value of a tensor is invariant are commonly known as rotations, although they also include mirroring.

A well known example of a scalar valued function of two vectors is the dot product, which depends on the length of the two vectors and the angle between them. A rotation changes none of these factors. Whereas a scale change will change the number used to measure each vector's length, and so must change the output. It's no different from deciding to work in yards rather than metres.

\section{Tensors as functions}

Sometimes a tensor is defined as a scalar valued function of vectors. It might be more accurate to say that any tensor \textit{can be employed} as such, i.e. a function that accepts $n$ vectors (given by its rank, $n$) and produces a scalar. However, in the same way, supposing its rank is high enough, it can be employed as a function that accepts $n-1$ vectors and produces a vector, or accepts $n-2$ vectors and produces a matrix, etc.

If $\boldsymbol{T}$ is a rank-3 tensor with coordinates $T_{ijk}$, and if we have three rank-1 tensors (vectors) with coordinates $A^i$, $B^j$ and $C^k$ we can evaluate $\boldsymbol{T}$ by summation notation:

$$
\boldsymbol{T(A,B,C)}
= T_{ijk}A^iB^jC^k
= \sum_i\sum_j\sum_kT_{ijk}A^iB^jC^k
$$

This is not purely the tensor product - we've also chosen to perform contraction three times, by repeating each of the three indices.

What happens if we only have two vectors available to us at the moment? We can partially apply the tensor:

$$
\boldsymbol{T(\_,B,C)}
= T_{ijk}B^jC^k
= \sum_j\sum_kT_{ijk}B^jC^k
= V_i
$$

Note that in the component summation, the index $i$ is not summed over - it's unbound, or free, and so the expression is a way to compute the $i$-th component of a vector $\boldsymbol{V}$. So in a sense, we have treated the rank-3 tensor as a vector-valued function of two vectors. Or if you prefer, a function of two vectors that produces a function of one vector.

But we can "finish the job" whenever we obtain $\boldsymbol{A}$:

$$
\boldsymbol{V(A)}
= V_{i}A^i
= \sum_i{V_iA^i}
$$

And we have no free indices, so the result is a scalar. This freedom to apply $\boldsymbol{T}$ to $\boldsymbol{B}$ and $\boldsymbol{C}$ first, and then apply the result of that to $\boldsymbol{A}$, shuffling the order of operations, is part of the essence of tensors. Ultimately all we are doing is multiplying sets of numbers, and then summing them up, so the order in which we do these things is up to us; it makes no difference to the result, \textit{as long as we are consistent} in matching subscript indices to superscript indices.

\section{Metric tensor}

To define the distance between two points in space, we can think of a vector reaching from one point to the other. So we just want to compute the length of that delta-vector, $\boldsymbol{D}$ from its coordinates.

In Euclidean space with an orthonormal basis, we use Pythagoras's theorem, which just means that we take the dot product of the vector with itself to get the squared length.

But the dot product is a contraction of rank-2 tensor resulting from the tensor product of the vector with itself. If we don't do the contraction, we get the square matrix $\boldsymbol{S}$:

$$S_{ij} = D_iD_j$$

The diagonal of $\boldsymbol{S}$ is just the squares of the coordinates, the ingredients needed for Pythagoras. The other elements are all possible combinations of the coordinates. We can use a matrix $g_{ij}$ to pick out the ingredients we want to include in our sum:

$$s = g_{ij}S^{ij}$$

So $g_{ij}$ is an example of a tensor being used as a function, specifically a scalar-valued function of two vectors. If we give it two different vectors it gives us the inner product of them, and if we give it the same vector twice, it gives us the square of the magnitude of that vector.

In Euclidean space, $g_{ij}$ is simply the Kroneker delta, so that only the diagonal elements are included in the sum. In curved space it has other values.

Note that there is much redundancy in such a matrix because $D_1D_2$ is the same as $D_2D_1$. For example, in a 4-dimensional space there are 16 matrix elements but only 10 are needed (the 4 of the diagonal and 6 from either side of the diagonal).
