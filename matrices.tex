\chapter{Summation, Indices and Matrices}

\section{Indices}

An index (plural: indices) is a subscript (and in some contexts a superscript) that stands for an integer.

$$a_n$$

This tells us that $a$ is not just one value, but several. The $n$ can be assumed to take a small range of values such as $1, 2, 3$, the exact size of this range depending on the situation. (In physics if we have a function of integers it's usually written as a set with an index like this, with $f(x)$ reserved for functions of continuous values.)

Instead of labelling spatial coordinates $x, y, z$, we can call them $x_1, x_2, x_3$ and avoid the need to repeat ourselves by just giving the rule for the behaviour of $x_n$, which is then unambiguously the same for all three dimensions of space.

Often we want to add the values:

$$x_1 + x_2 + x_3 + \dots + x_n$$

The shorthand for this is to use the $\Sigma$ symbol:

$$\sum_n{x_n} = x_1 + x_2 + x_3 + \dots + x_n$$

Later this will become so commonplace that we'll adopt an even shorter shorthand.


\section{Vectors and Matrices}

Thinking initially of vectors as mere collections of numbers (a viewpoint which we will rethink in ยง\ref{ch:vectors}), indices give us a way to talk about them. $x_n$ could represent a single row or column of $n$ numbers.

Likewise we can use two indices to label the numbers in a grid or matrix (plural: matrices). Given:

$$
M = \begin{bmatrix}
5 & 2 & 7 \\
0 & 1 & 0 \\
4 & 6 & 8
\end{bmatrix}
$$

We can refer to the elements of $M$ as $M_{ij}$, with $i$ giving the row and $j$ the column. So:

$$M_{32} = 6$$

This unfortunately looks a lot like the number $32$. When it's clear we aren't talking about raising numbers to powers, we use a combination of subscript and superscript indices, with superscripts meaning rows and subscripts meaning columns:

$$M^3_2 = 6$$

\section{Matrix multiplication} \label{sec:matrix-multiplication}

The simplest introduction to the purpose of a matrix is to consider a transformation that operates on the Euclidean plane, mapping any point given by coordinates $(p_1, p_2)$ to new positions given by $(q_1, q_2)$. We will restrict ourselves to transformations that can be expressed as:

\begin{equation}
\begin{split}
q_1 = M_{11}p_1 + M_{12}p_2 \\
q_2 = M_{21}p_1 + M_{22}p_2
\end{split}
\label{eqn:matrix-multiplication-raw}
\end{equation}

That is, we only choose four numbers $M_{ij} = (M_{11}, M_{12}, M_{21}, M_{22})$ to completely control the transformation. In matrix notation the above is:

\begin{equation}
\begin{split}
\begin{bmatrix}
q_1 \\
q_2
\end{bmatrix}
&=
\begin{bmatrix}
M_{11} & M_{12} \\
M_{21} & M_{22}
\end{bmatrix}
\begin{bmatrix}
p_1 \\
p_2
\end{bmatrix} \\
&=
M
\begin{bmatrix}
p_1 \\
p_2
\end{bmatrix}
\end{split}
\end{equation}

Or as a summation, where the $i$ and $j$ can take on the values $1$ or $2$:

\begin{equation}
q_i = \sum_{j} M_{ij} p_j
\label{eqn:matrix-multiplication-summation}
\end{equation}

What if we then apply to $(q_1, q_2)$ a second such transformation described by $N_{11}, N_{12}, N_{21}, N_{22}$ to get $(r_1, r_2)$? 

\begin{equation}
\begin{split}
\begin{bmatrix}
r_1 \\
r_2
\end{bmatrix}
&=
\begin{bmatrix}
N_{11} & N_{12} \\
N_{21} & N_{22}
\end{bmatrix}
\begin{bmatrix}
q_1 \\
q_2
\end{bmatrix} \\
&= 
\begin{bmatrix}
N_{11} & N_{12} \\
N_{21} & N_{22}
\end{bmatrix}
\left(
\begin{bmatrix}
M_{11} & M_{12} \\
M_{21} & M_{22}
\end{bmatrix}
\begin{bmatrix}
p_1 \\
p_2
\end{bmatrix}
\right)
\end{split}
\label{eqn:matrix-bracketing}
\end{equation}

By returning to the actual formulae and doing the substitution we can arrive at the answer, via a mess that ends up very simple. Doing the tedious work for $r_1$ alone:

\begin{equation}
\begin{split}
r_1 &= N_{11}(M_{11}p_1 + M_{12}p_2) + N_{12}(M_{21}p_1 + M_{22}p_2)  \\
    &= N_{11}M_{11}p_1 + N_{11}M_{12}p_2 + N_{12}M_{21}p_1 + N_{12}M_{22}p_2    \\
    &= (N_{11}M_{11} + N_{12}M_{21})p_1 + (N_{11}M_{12} + N_{12}M_{22})p_2    
\end{split}
\end{equation}

Doing the same for $r_2$, it turns out that applying these two transformations is like applying a single transformation given by another matrix $O$ that we can prepare directly from $M$ and $N$:

$$
\begin{bmatrix}
r_1 \\
r_2
\end{bmatrix}
= 
\begin{bmatrix}
N_{11}M_{11} + N_{12}M_{21} & N_{11}M_{12} + N_{12}M_{22} \\
N_{21}M_{11} + N_{22}M_{21} & N_{21}M_{12} + N_{21}M_{22}
\end{bmatrix}
\begin{bmatrix}
p_1 \\
p_2
\end{bmatrix}
$$

Which means that instead of bracketing how we did in \eqref{eqn:matrix-bracketing}, we can "multiply" the two matrices first:

\begin{equation}
\begin{bmatrix}
N_{11} & N_{12} \\
N_{21} & N_{22}
\end{bmatrix}
\begin{bmatrix}
M_{11} & M_{12} \\
M_{21} & M_{22}
\end{bmatrix}
=
\begin{bmatrix}
N_{11}M_{11} + N_{12}M_{21} & N_{11}M_{12} + N_{12}M_{22} \\
N_{21}M_{11} + N_{22}M_{21} & N_{21}M_{12} + N_{21}M_{22}
\end{bmatrix}
\label{eqn:matrix-combination}
\end{equation}

Generalising, if we're multiplying two matrices $M$ and $N$ to get $O$:

$$O = MN$$

in summation notation we can define the cell at row $i$ and column $j$ of $O$ as follows:

$$O_{ij} = \sum_k M_{ik} N_{kj}$$

This formula describes all the above examples of matrix multiplication. If $j$ is only allowed to take on the value 1 then $N$ and $O$ become column matrices with no need of a second index, and we've recreated \eqref{eqn:matrix-multiplication-summation} albeit with some light renaming.

Note that in general $MN \ne NM$, that is, multiplication is not necessarily commutative, because referring to the result of \eqref{eqn:matrix-combination} we can see there is no way to rearrange the terms to make the two results equal. However they can be made to match up if the matrices are symmetric, meaning that diagonally opposite elements are equal ($M_{ij} = M_{ji}$), or equivalently the matrix is equal to its transpose ($M = N^T$), so in that case there is commutativity.

But also matrix multiplication turns out to be merely one combination of some more basic concepts we'll return to in ยง\ref{tensor-contraction}.

\section{Kronecker Delta} \label{def:Kronecker}

One of the most important examples of a matrix is the \textit{identity} that makes no difference when it appears in a multiplication. Referring to \eqref{eqn:matrix-multiplication-raw}, we want $q_i = p_i$ and therefore the main diagonal elements $M_{ii}$ need to be $1$, and all others are $0$. This generalises to any size of square matrix, but is awkward to represent in a stretchy way:

$$
\hat{I} = \begin{bmatrix}
1 & 0 & 0 & \dots & 0 \\
0 & 1 & 0 & \dots & 0 \\
0 & 0 & 1 & \dots & 0 \\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0 & 0 & 0 & \dots & 1
\end{bmatrix}
$$

So instead we define the Kronecker delta, which has two indices representing row and column:

$$
\delta_{ij} = \begin{cases}
0 &\text{if } i \neq j,   \\
1 &\text{if } i=j.   \end{cases}
$$

Although in this case it's not important which index is the row and which the column, due to the symmetry of the identity matrix.

\section{Manipulating Summations}

As a summation is just a template for generating terms that added, the algebraic manipulations we casually use all the time for addition and multiplication are automatically available. Multiplication is distributive across addition, so:

$$
\sum_{n} x f(n) = x \sum_{n} f(n)
$$

where $x$ is a constant. Also, think about "multiplying out" two multi-term expressions:

$$
(a_1 + a_2 + a_3)(b_1 + b_2) = a_1b_1 + a_1b_2 + a_2b_1 + a_2b_2 + a_3b_1 + a_3b_2  
$$

So in a summation we can name two indices and this produces a term for every combination of their allowed values:

$$
\left( \sum_{i} a_i \right) \left( \sum_{j} b_j \right) = \sum_{ij} a_i b_j
$$

This works in both directions. If we initially have a summation expression:

$$
\sum_{kl} a_k b_l 
$$

and then we discover that $a_k$ is itself actually the result of another summation:

$$
a_k = \sum_{j} M_{kj}
$$

(and here, note that $k$ is acting like an integer parameter of a function), we can obviously perform a substitution:

$$
\sum_{kl} \left( \sum_{j} M_{kj} \right) b_l 
$$

Unfolding that, supposing all three indices may take the values $\{1, 2\}$:

\begin{equation}
    \begin{aligned}
        & (M_{11} b_1 + M_{12} b_1) + \\
        & (M_{21} b_1 + M_{22} b_1) + \\
        & (M_{11} b_2 + M_{12} b_2) + \\
        & (M_{21} b_2 + M_{22} b_2)            
    \end{aligned}
\end{equation}

Each of the four parenthesised pieces results from one expansion of the inner sum over $j$, but the parentheses can melt away leaving eight terms added together, which can be written as:

$$
\sum_{jkl} M_{kj} b_l
$$

But equally we could begin with the above and extract the concept of summing a slice of $M$ by inventing:

$$
a_k = \sum_{j} M_{kj}
$$

Which has the effect of removing the $j$ summation index, as it's hidden within the computation of $a_k$:

$$
\sum_{kl} a_k b_l 
$$

These may appear counter-intuitive at first glance but when you remember what summation expands into, they are quite obvious.
