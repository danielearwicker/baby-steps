\chapter{Geometric Algebra}

A general way of defining the product of two vectors, as the sum of two other kinds of product:

$$
\vec{u}\vec{v}
=
\vec{u} \cdot
\vec{v}
+
\vec{u}
\wedge
\vec{v}
$$

The first is the familiar dot product that produces a scalar. The second is the \textit{external product} and its result is something called a \textit{bivector}. The question naturally arises, how can we sum a scalar and an exotic new object? But we can brush over this question much as we do with complex numbers, in which we sum a real and imaginary component without requiring them to reduce to a single term.

Geometrically a vector is defined by three things:

\begin{itemize}
  \item its length (or \textit{magnitude}, a scalar)
  \item the line it sits in
  \item in which of the two available directions on the line it is pointing.
\end{itemize}

A bivector is a flat bounded surface only defined by three things:

\begin{itemize}
  \item its area (or \textit{magnitude}, a scalar)
  \item the plane it sits in (sometimes called \textit{attitude} or \textit{orientation})
  \item in which of the two available directions it faces (also sometimes called \textit{orientation}, perhaps better described as \textit{direction}), and also conceivable as a direction of rotation.
\end{itemize}

It follows that the exact shape of the boundary around the area of a bivector is irrelevant. To form a bivector from vectors $\vec{u}$ and $\vec{v}$, where $\vec{u}$ points to the right and $\vec{v}$ slants up and to the right, make a parallelogram from them, with sides: $\vec{u}$, $\vec{v}$, $-\vec{u}$, $-\vec{v}$. The cycle of vectors follows an anti-clockwise route, which characterises the direction of the bivector. Equivalently, by rotating $\vec{u}$ anti-clockwise we can align it with $\vec{v}$.

By the right-hand rule, if a bivector is anti-clockwise then it is pointing toward you, whereas if it is clockwise then it is pointing away from you. It has a "front" and a "back".

We can picture a circle (or any other shape) with the same area embedded in the same plane and imagine it rotating anti-clockwise, and this would be a way to picture the very same bivector.

A bivector with area zero is the zero bivector (whereupon the attitude and direction of rotation become meaningless.)

Another circle in the same plane with the same area, but rotating \textit{clockwise}, would be the negation of the first bivector. We can also construct this by tracing the parallelgram: $\vec{v}$, $\vec{u}$, $-\vec{v}$, $-\vec{u}$, i.e. having $\vec{u}$ and $\vec{v}$ switch places. Following the direction of the vectors we cycle clockwise.

Combining two vectors in this way to form a parallelogram is the \textit{external product}, and because if we exchange the vectors the product is negated, we say it is \textit{anticommutative}:

$$
\vec{u} \wedge \vec{v}
= -\vec{v} \wedge \vec{u}
$$

If $\vec{u}$ and $\vec{v}$ are colinear then the parallelogram would be of area zero, so the result is the zero bivector (so any vector squared is zero).

Returning to the full geometric product:

$$
\vec{u}\vec{v}
=
\vec{u} \cdot
\vec{v}
+
\vec{u}
\wedge
\vec{v}
$$

The product of a vector with itself is:

$$
\vec{u}^2
=\vec{u}\vec{u}
=
\vec{u} \cdot
\vec{u}
+
\vec{u}
\wedge
\vec{u}
$$

The dot product is just the scalar $|\vec{u}|^2$, and we just noted that the external product of any vector with itself is zero, so:

$$\vec{u}^2 = |\vec{u}|^2$$

It follows that in an orthonormal basis, the geometric product of any basis vector with itself is 1. The geometric product of two different basis vectors is a pure bivector with no scalar part, and (by anticommutativity) we can flip the sign by switching the two vectors.

Furthermore, because division is meaningful on scalars we can say for any vector $\vec{u}$:

$$\frac{ \vec{u}^2 }{ \vec{u}^2 } = 1$$

And so we can give a meaning to \textit{the inverse of a vector}:

$$\frac{ \vec{u} }{ \vec{u}^2 } = \vec{u}^{-1}$$

Multiplying by such an inverse is equivalent to division, but as always the order of the operands is important.

Due to the exterior product being anticommutative, if we switch vectors around the sign changes on the external product part:

$$
\vec{v}\vec{u}
= \vec{u} \cdot \vec{v} - \vec{u} \wedge \vec{v}
$$

First let's add the two orderings:

$$
\vec{v}\vec{u} + \vec{v}\vec{u}
= \vec{u} \cdot \vec{v} + \vec{u} \wedge \vec{v} + \vec{u} \cdot \vec{v} - \vec{u} \wedge \vec{v}
$$

So the external parts cancel leaving:

$$
\vec{v}\vec{u} + \vec{v}\vec{u}
= 2 \left[ \vec{u} \cdot \vec{v} \right]
$$

$$
\frac{\vec{v}\vec{u} + \vec{v}\vec{u}} {2}
= \vec{u} \cdot \vec{v}
$$

And so we have a way of computing the scalar product in terms of the geometric product.

Then we repeat this exercise but subtracting:

$$
\vec{v}\vec{u} - \vec{v}\vec{u}
= \vec{u} \cdot \vec{v} + \vec{u} \wedge \vec{v} - \vec{u} \cdot \vec{v} + \vec{u} \wedge \vec{v}
$$

This time the scalar parts cancel leaving:

$$
\vec{v}\vec{u} - \vec{v}\vec{u}
= 2\vec{u} \wedge \vec{v}
$$

$$
\frac{\vec{v}\vec{u} - \vec{v}\vec{u}} {2}
= \vec{u} \wedge \vec{v}
$$

And similarly we have a way of computing the external product in terms of the geometric product.

Alternatively we can write $\vec{u}$ and $\vec{v}$ in terms of an orthonormal vector basis $e_i$ with scalar coordinates $u_i$ and $v_i$, and write down the product $\vec{u}\vec{v}$:

$$
\vec{u}\vec{v} = \sum_i{u_i e_i} \sum_j{v_j e_j}
= \sum_{ij}{u_i v_j e_i e_j}
$$

We can arrange this in a square matrix of terms, rows $i$ and columns $j$:

$$
M_{ij} = {u_i v_j e_i e_j}
$$

The members on the diagonal (where $i=j$) are just $u_i u_i e_i^2$ and we know that the geometric square of a unit vector is 1, so the sum of the diagonal terms on their own (known as the \textit{trace} of $M$) is just the dot product.

The other terms of the sum include diagonally opposite pairs such as:

$$v_1u_2\vec{e_1}\vec{e_2} + v_2u_1\vec{e_2}\vec{e_1}$$

We know that $\vec{e_1}\vec{e_2} = -\vec{e_2}\vec{e_1}$, so the above can be written as:

$$
v_1u_2\vec{e_1}\vec{e_2} - v_2u_1\vec{e_1}\vec{e_2}
= (v_1u_2 - v_2u_1)\vec{e_1}\vec{e_2}
$$

In three dimensions there are three such pairs:

$$
(u_1v_2 - u_2v_1)\vec{e_1}\vec{e_2} +
(u_2v_3 - u_3v_2)\vec{e_2}\vec{e_3} +
(u_1v_3 - u_3v_1)\vec{e_1}\vec{e_3}
$$

Weirdly, this is the formula for the cross product $\vec{u} \times \vec{v}$ but with a slight difference:

$$
\vec{u} \times \vec{v} = (u_1v_2 - u_2v_1)\vec{e_3} +
(u_2v_3 - u_3v_2)\vec{e_1} +
(u_1v_3 - u_3v_1)\vec{e_2}
$$

In fact if we multiply the cross product by $\vec{e_1}\vec{e_2}\vec{e_3}$ we recover the external product, because for example in $\vec{e_1}\vec{e_2}\vec{e_3}\vec{e_3}$ the repeated $\vec{e_3}$ factors equal $1$, so in each term we replace the basis vector with the (bivector) product of the other two basis vectors.

In each term we've written a bivector in place of a vector perpendicular to it. Physicists often represent a rotation with a vector perpendicular to the plane of rotation, using the right-hand rule to relate the direction of rotation to the direction of the vector (a vector pointing away from you represents clockwise rotation from your perspective). It's known as a \textit{pseudovector} because in physics the term vector is restricted to describing objects that transform in the expected way under a change of basis, and such pseudovectors do not.

But if we avoid literal pseudovectors and stick with bivectors then this problem is avoided. A bivector is a rank 2 tensor and as such it transforms correctly. In physics all uses of the cross product would be better represented as producing a bivector.

Furthermore, if you think of a rank N tensor as a function that accepts a vector and produces a rank N-1 tensor, how does that relate to multivectors?

A bivector $A$ multiplied by a vector $\vec{w}$ is like the product of two vectors $\vec{u}\vec{v}$ multiplied by a vector:

$$A\vec{w} = \vec{u}\vec{v}\vec{w} $$

Any vector $\vec{u}$ can be written as a weighted sum of the orthogonal basis vectors, the weightings $u_i$ being the components of the vector:

$$\vec{u} = \sum_i{u_i e_i}$$

$$A\vec{w} = \sum_{ijk}{u_i v_j w_k e_i e_j e_k}$$

This isn't a square matrix, but it is a cube-shaped object. It's a trivector, which as we will see is not a scalar but in $\mathbb{R}^3$ has only one available "orientation" (just like a bivector in $\mathbb{R}^2$ only has one plane to lie in) so the only properties that we can use to distinguish two trivectors are the magnitude and sign, so in $\mathbb{R}^3$ a trivector is a pseudoscalar.

As the geometric product is the sum of the dot product (which we found on the diagonal of our matrix) and the external product (which we found in all the other members), we can think of the geometric product as the sum of two matrices:

$$
\vec{u} \vec{v} =
\begin{bmatrix}
u_1 v_1 & 0 & 0 \\
0 & u_2 v_2 & 0 \\
0 & 0 & u_3 v_3
\end{bmatrix}
+
\begin{bmatrix}
0 & u_1 v_2 & u_1 v_3 \\
u_2 v_1 & 0 & u_2 v_3 \\
u_3 v_1 & u_3 v_2 & 0
\end{bmatrix}
$$

Just as a bivector is an area with an orientation, a \textit{trivector} is a volume with an orientation.

But note that this orientation is not like the ability of some specifically-shaped solid to be rotated in $\mathbb{R}^3$.

\begin{itemize}
    \item A trivector has no shape, so for simplicity visualise it as a sphere, and note that the normal sense of rotation will have no effect on it, and cannot be a distinguishing property. So that's not what we mean by orientation in this context.
    \item Consider how a bivector in $\mathbb{R}^2$ only has one choice of plane to lie in, whereas in $\mathbb{R}^3$ it can choose from an infinity of planes akin to $\mathbb{R}^2$ that are embedded in $\mathbb{R}^3$. The bivector will have no extent orthogonal to its chosen plane, but only exists as an area within that plane.
    \item Analogously a trivector in $\mathbb{R}^3$ has no choice about what space its volume exists in, but in $\mathbb{R}^4$ there is an infinity of possible hyperplanes (three dimensional "slices" of $\mathbb{R}^4$) to choose from. A trivector exists as a volume in only one such hyperplane, and has no extent in the remaining orthogonal direction.
    \item For a trivector orientation only becomes a distinguishing property in $\mathbb{R}^4$ or higher.
\end{itemize}

A trivector can be constructed as the product of three vectors, producing a parallelopiped, a six-faced figure. Each face is a bivector (just as each side of a bivector is a vector). Higher-dimensioned forms follow similarly. All of these are known as \textit{k-vectors}.

A linear combination of orthonormal basis k-vectors is called a multivector. So in two dimensions where we have basis vectors $\vec{e_1}$, $\vec{e_2}$, the available k-vectors are:

\begin{itemize}    
    \item scalars (multiples of 1)
    \item multiples of $\vec{e_1}$
    \item multiples of $\vec{e_2}$
    \item multiples of the bivector $\vec{e_1}\vec{e_2}$
\end{itemize}

So in two dimensions all multivectors $x$ are described by 4 components, $x_i$:

$$x_1 + x_2\vec{e_1} + x_3\vec{e_2} + x_4\vec{e_1}\vec{e_2}$$

In three dimensions we have:

\begin{itemize}    
    \item scalars (multiples of 1)
    \item multiples of $\vec{e_1}$
    \item multiples of $\vec{e_2}$
    \item multiples of $\vec{e_3}$
    \item multiples of the bivector $\vec{e_1}\vec{e_2}$
    \item multiples of the bivector $\vec{e_2}\vec{e_3}$
    \item multiples of the bivector $\vec{e_1}\vec{e_3}$
    \item multiples of the trivector $\vec{e_1}\vec{e_2}\vec{e_3}$
\end{itemize}

and therefore 8 components in a multivector:

$$
x_1 + x_2\vec{e_1} + x_3\vec{e_2} + x_4\vec{e_3}
+ x_5\vec{e_1}\vec{e_2}
+ x_6\vec{e_2}\vec{e_3}
+ x_7\vec{e_1}\vec{e_3}
+ x_8\vec{e_1}\vec{e_2}\vec{e_3}
$$

Note how in each case there is one term made of all the vectors:

\begin{itemize} 
    \item $x_4\vec{e_1}\vec{e_2}$ (2 dimensions)
    \item $x_8\vec{e_1}\vec{e_2}\vec{e_3}$ (3 dimensions)
\end{itemize}

so there is only one component for it. This makes it similar to a scalar and it's known as a pseudoscalar.

Also in $N$ dimensions there is a set of terms made of $N-1$ vectors, i.e. using all basis vectors except one, and there are $N$ such components, just like a vector:

\begin{itemize}
    \item $x_2\vec{e_1} + x_3\vec{e_2}$ (2 dimensions)
    \item $x_5\vec{e_1}\vec{e_2} + x_6\vec{e_2}\vec{e_3} + x_7\vec{e_1}\vec{e_3}$ (3 dimensions)
\end{itemize}

so these together form a pseudovector.

These terms that are the products of vectors are called \textit{blades}. The number of vectors that go into making blade is call the \textit{grade}. A 0-blade is a scalar, and in $N$-dimensions an $N$-blade is a pseudoscalar, whereas $(N-1)$-blade is a pseudovector.

Even when constrained to two dimensions, with orthonormal basis $\vec{e_1}$, $\vec{e_2}$ bivectors have their uses. The bivector $\vec{e_1}\vec{e_2}$, which can be pictured as a square with a cycle of vectors $\vec{e_1}$, $\vec{e_2}$, $-\vec{e_1}$, $-\vec{e_2}$ around its edge, can be multiplied by itself via the geometric product:

$$
(\vec{e_1}\vec{e_2})^2
= \vec{e_1}\vec{e_2}\vec{e_1}\vec{e_2}
= -\vec{e_2}\vec{e_1}\vec{e_1}\vec{e_2}
= -\vec{e_2}\vec{e_2}
= -1
$$

So the bivector $\vec{e_1}\vec{e_2}$ gives the value $-1$ when squared, which means that in geometric algebra, we have a way of defining $i$ for any given plane: the product of two orthogonal unit vectors in that plane.

As usual, it represents a $\pi/2$ or $90$-degree rotation, but with one difference due to the anticommutative nature of the geometric product: $\vec{a}i$ rotates clockwise, whereas $i\vec{a}$ rotates anti-clockwise.

As a bivector is only characterised by its magnitude (area), the plane it lies in (in this case we're only considering two dimensions so there's no freedom here), and the direction of rotation (clockwise or anti-clockwise), this latter being equivalent to a change of sign, all bivectors in the plane can be written as scalar-multiples of the unit bivector $i$. So in fact in two dimensions any multivector is the sum of a scalar and a scalar-multiple of $i$. This means that a complex number can be viewed as a multi-vector in $\mathbb{R}^2$:

$$a+bi$$

Furthermore a change to the sign of the "imaginary part" is the same as the complex conjugate, and this is what happens when a geometric product has its operands switched.

Note that we normally think of $i$ as a scalar, and therefore it is no coincidence that we've discovered that it is actually a pseudoscalar, a blade (in N dimensions, an N-vector that is the product of N 1-vectors).

Much of what has been said here about 2 dimensions holds true in 3. The highest grade is the trivector $\vec{e_1}\vec{e_2}\vec{e_3}$ and this again serves as the pseudoscalar and is labelled $i$, and $i^2=1$ as before.

But unlike 2 dimensions, in 3 dimensions $i$ commutes with all multivectors:

$$iA = Ai$$

We can also think of the wedge product as a way to characterise the non-commutativity of two vectors under the tensor products.

$$\vec{u} \wedge \vec{v} = \vec{u} \otimes \vec{v} - \vec{v} \otimes \vec{u}$$

So the matrix element at $i, j$ is given by:

$$(\vec{u} \wedge \vec{v})_{ij} = u_i v_j - u_j v_i$$

